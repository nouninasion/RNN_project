# Shakespearean Text Generator
## Table of Contents

  - [Project Overview](https://github.com/nouninasion/RNN_project/blob/main/README.md#project-overview)
  - [How it Works: Recurrent Neural Networks (RNNs)](https://github.com/nouninasion/RNN_project/blob/main/README.md#how-it-works-recurrent-neural-networks-rnns)
      - [Long Short-Term Memory (LSTM)](https://github.com/nouninasion/RNN_project/blob/main/README.md#long-short-term-memory-lstm)
  - [Dataset](https://github.com/nouninasion/RNN_project/blob/main/README.md#dataset)
  - [Preprocessing](https://github.com/nouninasion/RNN_project/blob/main/README.md#preprocessing)
  - [Model Architecture](https://github.com/nouninasion/RNN_project/blob/main/README.md#model-architecture)
  - [Text Generation](https://github.com/nouninasion/RNN_project/blob/main/README.md#text-generation)
  - [Getting Started](https://github.com/nouninasion/RNN_project/blob/main/README.md#getting-started)
  - [Libraries Used](https://github.com/nouninasion/RNN_project/blob/main/README.md#libraries-used)
  - [Example Output](https://github.com/nouninasion/RNN_project/blob/main/README.md#example-output)

## Project Overview

This project implements a deep learning model to generate sequential text. By feeding the model a large corpus of text (in this case, Shakespeare's works), it learns the patterns, grammar, and style of the writing, enabling it to produce new, coherent, and contextually relevant sequences of characters.

## How it Works: Recurrent Neural Networks (RNNs)

Recurrent Neural Networks (RNNs) are a class of neural networks designed for processing sequential data. Unlike traditional feedforward neural networks, RNNs have a "memory" that allows them to process sequences by taking into account previous inputs in the sequence. This makes them particularly well-suited for tasks involving time-series data, natural language processing, and, as in this project, text generation.

The key characteristic of an RNN is its recurrent connection, which allows information to persist from one step of the sequence to the next. At each time step, an RNN layer takes an input and the hidden state from the previous time step, producing an output and an updated hidden state. This hidden state acts as a memory of the sequence processed so far.

### Long Short-Term Memory (LSTM)

While standard RNNs are good at learning short-term dependencies, they often struggle with long-term dependencies due to the vanishing gradient problem. Long Short-Term Memory (LSTM) networks are a special type of RNN designed to overcome this limitation. LSTMs introduce a "cell state" and various "gates" (input gate, forget gate, and output gate) that regulate the flow of information into and out of the cell state. This gating mechanism allows LSTMs to selectively remember or forget information over long sequences, making them highly effective for complex sequence-to-sequence tasks like text generation.

## Dataset

The model is trained on a portion of Shakespeare's complete works, downloaded directly from a TensorFlow storage URL. The specific text used is a slice from character index 300,000 to 800,000 of the original corpus, converted to lowercase. This segment provides a rich and consistent style for the model to learn from.

## Preprocessing

The preprocessing steps prepare the raw text data for the neural network:

  - **Text Loading and Slicing**: The text is loaded and a specific portion is selected.
  - **Lowercasing**: All text is converted to lowercase to reduce the vocabulary size and treat 'A' and 'a' as the same character.
  - **Vocabulary Creation**: A sorted list of unique characters from the text slice forms the vocabulary.
  - **Character-to-Index Mapping**: Two dictionaries are created: `char_to_index` maps each character to a unique integer ID, and `index_to_char` does the reverse. This is essential for converting text to numerical data and back.
  - **Sequence Generation (Commented Out)**: The notebook includes commented-out code that demonstrates how the training data (`X` and `y`) would be prepared.
      - `SEQ_LENGTH` (40): Each input sequence fed to the RNN consists of 40 characters.
      - `STEP_SIZE` (3): New sequences are generated by shifting the window by 3 characters, creating overlapping sequences to increase the dataset size and capture more context.
      - **One-Hot Encoding**: Input sequences (`X`) and target characters (`y`) are converted into a one-hot encoded NumPy arrays. This means each character is represented as a binary vector of the size of the vocabulary, with a '1' at the index corresponding to the character and '0's elsewhere.

## Model Architecture

The model uses a Sequential Keras model with an LSTM layer followed by a Dense layer and a Softmax activation.

  - **LSTM Layer (128 units)**: This is the core recurrent layer responsible for learning long-term dependencies in the text. It processes the input sequences and maintains a hidden state.
  - **Dense Layer**: A fully connected layer that transforms the output of the LSTM layer into a vector with the size of the vocabulary.
  - **Softmax Activation**: This activation function converts the raw outputs of the Dense layer into probability distributions over the vocabulary, indicating the likelihood of each character being the next in the sequence.

The notebook loads a pre-trained model named `textgenerator.keras` which implies that the training process for this specific model has already been completed externally.

## Text Generation

The `generate_text` function handles the text generation process:

1.  **Seed Sentence Selection**: A random starting sequence (`SEQ_LENGTH` characters long) is chosen from the original text.
2.  **Iterative Prediction**:
      - The seed sentence is one-hot encoded and fed into the pre-trained `model`.
      - The model predicts the probability distribution of the `next_character`.
      - A `sample` function is used to select the next character based on these probabilities and a `temperature` parameter.
      - The `temperature` parameter controls the randomness of the generation:
          - Lower temperatures (e.g., 0.2) make the model more confident and deterministic, often leading to more common and "safe" characters, which can result in repetitive or less creative text.
          - Higher temperatures (e.g., 1.0) increase the randomness, making the model take more risks and generate more surprising or creative text, though it might also lead to less coherent or grammatically incorrect output.
      - The predicted `next_character` is appended to the `generated` text, and the `sentence` is updated by shifting one character to the left and adding the new character to the right.
3.  **Output**: The function returns the generated text of the specified `length`.

The notebook then demonstrates text generation at different temperatures (0.2, 0.4, 0.6, 0.8, 1.0) to showcase the effect of temperature on creativity and coherence.

## Getting Started

To run this notebook, you will need Google Colab or a local Jupyter environment with the following libraries installed:

  - `tensorflow`
  - `keras` (usually comes with TensorFlow)
  - `numpy`
  - `random` (Python built-in)

You can install these using pip:

```bash
pip install tensorflow numpy
```

To use this notebook, you will need to:

1.  Download the `textgenerator.keras` pre-trained model file and place it in the same directory as your notebook. (The code directly loads this file).
2.  Open and run the `LaptopPricePrediction.ipynb` file in your Google Colab or Jupyter environment.

## Libraries Used

  - `random`
  - `numpy`
  - `tensorflow`
  - `keras.models`
  - `keras.layers`
  - `keras.optimizers`

## Example Output

Here are examples of text generated by the model at different temperatures (output will vary due to randomness):

**---------0.2---------**
[Actual generated text will appear here when run]
...
**---------0.4---------**
[Actual generated text will appear here when run]
...
**---------0.6---------**
[Actual generated text will appear here when run]
...
**---------0.8---------**
[Actual generated text will appear here when run]
...
**---------1.0---------**
[Actual generated text will appear here when run]
...
